{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuto feature engineering from kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ks = pd.read_csv('../input/kickstarter-projects/ks-projects-201801.csv',\n",
    "                 parse_dates=['deadline', 'launched'])\n",
    "ks.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop live projects\n",
    "ks = ks.query('state != \"live\"')\n",
    "\n",
    "# Add outcome column, \"successful\" == 1, others are 0\n",
    "ks = ks.assign(outcome=(ks['state'] == 'successful').astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = ks.assign(hour=ks.launched.dt.hour,\n",
    "               day=ks.launched.dt.day,\n",
    "               month=ks.launched.dt.month,\n",
    "               year=ks.launched.dt.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "cat_features = ['category', 'currency', 'country']\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Apply the label encoder to each column\n",
    "encoded = ks[cat_features].apply(encoder.fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since ks and encoded have the same index and I can easily join them\n",
    "data = ks[['goal', 'hour', 'day', 'month', 'year', 'outcome']].join(encoded)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_fraction = 0.1\n",
    "valid_size = int(len(data) * valid_fraction)\n",
    "\n",
    "train = data[:-2 * valid_size]\n",
    "valid = data[-2 * valid_size:-valid_size]\n",
    "test = data[-valid_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "feature_cols = train.columns.drop('outcome')\n",
    "\n",
    "dtrain = lgb.Dataset(train[feature_cols], label=train['outcome'])\n",
    "dvalid = lgb.Dataset(valid[feature_cols], label=valid['outcome'])\n",
    "\n",
    "param = {'num_leaves': 64, 'objective': 'binary'}\n",
    "param['metric'] = 'auc'\n",
    "num_round = 1000\n",
    "bst = lgb.train(param, dtrain, num_round, valid_sets=[dvalid], early_stopping_rounds=10, verbose_eval=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "ypred = bst.predict(test[feature_cols])\n",
    "score = metrics.roc_auc_score(test['outcome'], ypred)\n",
    "\n",
    "print(f\"Test AUC score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical encodings\n",
    "some clever ways to work with categorical variables.7\n",
    "\n",
    "most basic encodings: one-hot encoding and label encoding\n",
    "\n",
    "learn about count encoding, target encoding, and CatBoost encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_splits(dataframe, valid_fraction=0.1):\n",
    "    valid_fraction = 0.1\n",
    "    valid_size = int(len(dataframe) * valid_fraction)\n",
    "\n",
    "    train = dataframe[:-valid_size * 2]\n",
    "    # valid size == test size, last two sections of the data\n",
    "    valid = dataframe[-valid_size * 2:-valid_size]\n",
    "    test = dataframe[-valid_size:]\n",
    "    \n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train, valid):\n",
    "    feature_cols = train.columns.drop('outcome')\n",
    "\n",
    "    dtrain = lgb.Dataset(train[feature_cols], label=train['outcome'])\n",
    "    dvalid = lgb.Dataset(valid[feature_cols], label=valid['outcome'])\n",
    "\n",
    "    param = {'num_leaves': 64, 'objective': 'binary', \n",
    "             'metric': 'auc', 'seed': 7}\n",
    "    bst = lgb.train(param, dtrain, num_boost_round=1000, valid_sets=[dvalid], \n",
    "                    early_stopping_rounds=10, verbose_eval=False)\n",
    "\n",
    "    valid_pred = bst.predict(valid[feature_cols])\n",
    "    valid_score = metrics.roc_auc_score(valid['outcome'], valid_pred)\n",
    "    print(f\"Validation AUC score: {valid_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "cat_features = ['category', 'currency', 'country']\n",
    "\n",
    "# Create the encoder\n",
    "count_enc = ce.CountEncoder()\n",
    "\n",
    "# Transform the features, rename the columns with the _count suffix, and join to dataframe\n",
    "count_encoded = count_enc.fit_transform(ks[cat_features])\n",
    "data = data.join(count_encoded.add_suffix(\"_count\"))\n",
    "\n",
    "# Train a model \n",
    "train, valid, test = get_data_splits(data)\n",
    "train_model(train, valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target encodings\n",
    "\n",
    "This technique uses the targets to create new features. So including the validation or test data in the target encodings would be a form of target leakage. Instead, you should learn the target encodings from the training dataset only and apply it to the other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the encoder\n",
    "target_enc = ce.TargetEncoder(cols=cat_features)\n",
    "target_enc.fit(train[cat_features], train['outcome'])\n",
    "\n",
    "# Transform the features, rename the columns with _target suffix, and join to dataframe\n",
    "train_TE = train.join(target_enc.transform(train[cat_features]).add_suffix('_target'))\n",
    "valid_TE = valid.join(target_enc.transform(valid[cat_features]).add_suffix('_target'))\n",
    "\n",
    "# Train a model\n",
    "train_model(train_TE, valid_TE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost encodings\n",
    "\n",
    "Finally, well look at CatBoost encoding. This is similar to target encoding in that its based on the target probablity for a given value. However with CatBoost, for each row, the target probability is calculated only from the rows before it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the encoder\n",
    "target_enc = ce.CatBoostEncoder(cols=cat_features)\n",
    "target_enc.fit(train[cat_features], train['outcome'])\n",
    "\n",
    "# Transform the features, rename columns with _cb suffix, and join to dataframe\n",
    "train_CBE = train.join(target_enc.transform(train[cat_features]).add_suffix('_cb'))\n",
    "valid_CBE = valid.join(target_enc.transform(valid[cat_features]).add_suffix('_cb'))\n",
    "\n",
    "# Train a model\n",
    "train_model(train_CBE, valid_CBE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution: Target encoding attempts to measure the population mean of the target for each level in a categorical feature. This means when there is less data per level, the estimated mean will be further away from the \"true\" mean, there will be more variance. There is little data per IP address so it's likely that the estimates are much noisier than for the other features. The model will rely heavily on this feature since it is extremely predictive. This causes it to make fewer splits on other features, and those features are fit on just the errors left over accounting for IP address. So, the model will perform very poorly when seeing new IP addresses that weren't in the training data (which is likely most new data). Going forward, we'll leave out the IP feature when trying different encodings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactions\n",
    "One of the easiest ways to create new features is by combining categorical variables. For example, if one record has the country \"CA\" and category \"Music\", you can create a new value \"CA_Music\". This is a new categorical feature that can provide information about correlations between categorical variables. This type of feature is typically called an interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = ks['category'] + \"_\" + ks['country']\n",
    "print(interactions.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_enc = LabelEncoder()\n",
    "data_interaction = baseline_data.assign(category_country=label_enc.fit_transform(interactions))\n",
    "data_interaction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = ['ip', 'app', 'device', 'os', 'channel']\n",
    "interactions = pd.DataFrame(index=clicks.index)\n",
    "for col1, col2 in itertools.combinations(cat_features, 2):\n",
    "    new_col_name = '_'.join([col1, col2])\n",
    "\n",
    "    # Convert to strings and combine\n",
    "    new_values = clicks[col1].map(str) + \"_\" + clicks[col2].map(str) ###CONVERT VALUES TO STRINGS IN DATAFRAME !\n",
    "\n",
    "    encoder = preprocessing.LabelEncoder()\n",
    "    interactions[new_col_name] = encoder.fit_transform(new_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count numbers sums etc\n",
    "\n",
    "Next, we'll count the number of projects launched in the preceeding week for each record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create a Series with a timestamp index\n",
    "launched = pd.Series(ks.index, index=ks.launched, name=\"count_7_days\").sort_index()\n",
    "launched.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.plotting import register_matplotlib_converters ## plot pandas frames?\n",
    "register_matplotlib_converters()\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_7_days = launched.rolling('7d').count() - 1 # !!!! ROLLING TRES COOL\n",
    "print(count_7_days.head(20))\n",
    "\n",
    "# Ignore records with broken launch dates\n",
    "plt.plot(count_7_days[7:]);\n",
    "plt.title(\"Number of projects launched over periods of 7 days\"); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_7_days.index = launched.values\n",
    "count_7_days = count_7_days.reindex(ks.index)\n",
    "\n",
    "baseline_data.join(count_7_days).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_past_events(series):\n",
    "        series = pd.Series(series.index, index=series)\n",
    "        # Subtract 1 so the current event isn't counted\n",
    "        past_events = series.rolling('6h').count() - 1\n",
    "        return past_events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of previous app downloads\n",
    "It's likely that if a visitor downloaded an app previously, it'll affect the likelihood they'll download one again. Implement a function previous_attributions that returns a Series with the number of times an app has been downloaded ('is_attributed' == 1) before the current event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pas sûr compris..\n",
    "\n",
    "def previous_attributions(series):\n",
    "    \"\"\"Returns a series with the number of times an app has been downloaded.\"\"\"\n",
    "    # Subtracting raw values so I don't count the current event\n",
    "    sums = series.expanding(min_periods=2).sum() - series\n",
    "    return sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time since last project\n",
    "Do projects in the same category compete for donors? If you're trying to fund a video game and another game project was just launched, you might not get as much money"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_since_last_project(series):\n",
    "    # Return the time in hours\n",
    "    return series.diff().dt.total_seconds() / 3600. #### HERE USEFUL !!\n",
    "\n",
    "df = ks[['category', 'launched']].sort_values('launched')\n",
    "timedeltas = df.groupby('category').transform(time_since_last_project) ## TRANSFORM FUNCTION !\n",
    "timedeltas.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get NaNs here for projects that are the first in their category. We'll need to fill those in with something like the mean or median. We'll also need to reset the index so we can join it with the other data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final time since last project\n",
    "timedeltas = timedeltas.fillna(timedeltas.median()).reindex(baseline_data.index) ### REINDEX\n",
    "timedeltas.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timedeltas = clicks.groupby('ip')['click_time'].transform(time_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_diff(series):\n",
    "    \"\"\"Returns a series with the time since the last timestamp in seconds.\"\"\"\n",
    "    return series.diff().dt.total_seconds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform numerical values\n",
    "Some models work better when the features are normally distributed, so it might help to transform the goal values. Common choices for this are the square root and natural logarithm. These transformations can also help constrain outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ks.goal, range=(0, 100000), bins=50);\n",
    "plt.title('Goal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.sqrt(ks.goal), range=(0, 400), bins=50);\n",
    "plt.title('Sqrt(Goal)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.log(ks.goal), range=(0, 25), bins=50);\n",
    "plt.title('Log(Goal)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Feature selection\n",
    "From the scikit-learn feature selection module, feature_selection.SelectKBest returns the K best features given some scoring function. For our classification problem, the module provides three different scoring functions:  χ2 , ANOVA F-value, and the mutual information score. The F-value measures the linear dependency between the feature variable and the target. This means the score might underestimate the relation between a feature and the target if the relationship is nonlinear. The mutual information score is nonparametric and so can capture nonlinear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "feature_cols = baseline_data.columns.drop('outcome')\n",
    "\n",
    "# Keep 5 features\n",
    "selector = SelectKBest(f_classif, k=5)\n",
    "\n",
    "X_new = selector.fit_transform(baseline_data[feature_cols], baseline_data['outcome'])\n",
    "X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've done something wrong here. The statistical tests are calculated using all of the data. This means information from the validation and test sets could influence the features we keep, introducing a source of leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = baseline_data.columns.drop('outcome')\n",
    "train, valid, _ = get_data_splits(baseline_data)\n",
    "\n",
    "# Keep 5 features\n",
    "selector = SelectKBest(f_classif, k=5)\n",
    "\n",
    "X_new = selector.fit_transform(train[feature_cols], train['outcome'])\n",
    "X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get back the features we've kept, zero out all other features\n",
    "selected_features = pd.DataFrame(selector.inverse_transform(X_new), \n",
    "                                 index=train.index, \n",
    "                                 columns=feature_cols)\n",
    "selected_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropped columns have values of all 0s, so var is 0, drop them\n",
    "selected_columns = selected_features.columns[selected_features.var() != 0]\n",
    "\n",
    "# Get the valid dataset with the selected features.\n",
    "valid[selected_columns].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 Regularization\n",
    "For regression problems you can use sklearn.linear_model.Lasso, or sklearn.linear_model.LogisticRegression for classification.\n",
    "These can be used along with sklearn.feature_selection.SelectFromModel to select the non-zero coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "train, valid, _ = get_data_splits(baseline_data)\n",
    "\n",
    "X, y = train[train.columns.drop(\"outcome\")], train['outcome']\n",
    "\n",
    "# Set the regularization parameter C=1\n",
    "logistic = LogisticRegression(C=1, penalty=\"l1\", solver='liblinear', random_state=7).fit(X, y)\n",
    "model = SelectFromModel(logistic, prefit=True)\n",
    "\n",
    "X_new = model.transform(X)\n",
    "X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get back the kept features as a DataFrame with dropped columns as all 0s\n",
    "selected_features = pd.DataFrame(model.inverse_transform(X_new), \n",
    "                                 index=X.index,\n",
    "                                 columns=X.columns)\n",
    "\n",
    "# Dropped columns have values of all 0s, keep other columns \n",
    "selected_columns = selected_features.columns[selected_features.var() != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, feature selection with L1 regularization is more powerful the univariate tests, but it can also be very slow when you have a lot of data and a lot of features. Univariate tests will be much faster on large datasets, but also will likely perform worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks = pd.read_parquet('../input/feature-engineering-data/baseline_data.pqt')\n",
    "data_files = ['count_encodings.pqt',\n",
    "              'catboost_encodings.pqt',\n",
    "              'interactions.pqt',\n",
    "              'past_6hr_events.pqt',\n",
    "              'downloads.pqt',\n",
    "              'time_deltas.pqt',\n",
    "              'svd_encodings.pqt']\n",
    "data_root = '../input/feature-engineering-data'\n",
    "for file in data_files:\n",
    "    features = pd.read_parquet(os.path.join(data_root, file))\n",
    "    clicks = clicks.join(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the best value of K, you can fit multiple models with increasing values of K, then choose the smallest K with validation score above some threshold or some other criteria. A good way to do this is loop over values of K and record the validation scores for each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features_l1(X, y):\n",
    "    logistic = LogisticRegression(C=0.1, penalty=\"l1\", random_state=7, solver='liblinear').fit(X, y)\n",
    "    model = SelectFromModel(logistic, prefit=True)\n",
    "\n",
    "    X_new = model.transform(X)\n",
    "\n",
    "    # Get back the kept features as a DataFrame with dropped columns as all 0s\n",
    "    selected_features = pd.DataFrame(model.inverse_transform(X_new),\n",
    "                                    index=X.index,\n",
    "                                    columns=X.columns)\n",
    "\n",
    "    # Dropped columns have values of all 0s, keep other columns\n",
    "    cols_to_keep = selected_features.columns[selected_features.var() != 0]\n",
    "\n",
    "    return cols_to_keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're using a tree-based model, using another tree-based model for feature selection might produce better results. What would you do different to select the features using a trees classifier?\n",
    "\n",
    "You could use something like RandomForestClassifier or ExtraTreesClassifier to find feature importances. SelectFromModel can use the feature importances to find the best features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
